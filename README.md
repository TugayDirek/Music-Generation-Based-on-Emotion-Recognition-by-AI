# Music-Generation-Based-on-Emotion-Recognition
The project consists of two different components which are emotion recognition from an input image and music generated based on the input image.
The dataset I used can be found in the following link as https://github.com/katoch99/Emotion-Recognition/tree/master/train. Since I used google colab for the project, I uploaded the pictures to my drive account. Later, I mounted the drive to the corresponding python file and used this data to train the CNN model. The model contains 5 CNN layers and 5 fully connected hidden layers after CNN layers. As a result of training, I obtained the trained model. I save this model in my google drive account and load it when I need to predict the emotion in the image without needing a new training since it takes 50 minutes to train on average. When I give a new image for classification, I take the probabilities of the detected emotions. The maximum probability in the output is taken as a result and the emotion of the person in the image can be recognized by this way. The input image for the emotion recognition is taken directly from the drive folder.

In the second part of the project, music is generated based on the output of the previous task. RNN must be trained by some audios which reflect the emotions in the song. The data which reflects the corresponding emotions in the previous task (happy, sad, angry and neutral) must be fed into the RNN as music data. For this purpose, I found the following dataset as https://www.kaggle.com/datasets/blaler/turkish-music-emotion-dataset. This dataset consists of .mp3 audio files which reflect happy, sad, angry and relaxed emotions. Each audio file is taken from a Turkish song. As the next step, these files must be given as training data to the RNN. For music generation, I followed the procedures in the following link as https://www.tensorflow.org/tutorials/audio/music_generation. Here, the RNN model predicts the next note based on the previous sequence of notes. It takes midi files as input. Since the data I found is mp3 files, I firstly converted these files to midi files. Briefly, notes in midi data consist of 3 different components (pitch, step and duration) which represents the corresponding note. These components are represented as numbers and these are given as input and output in the model. Later, the predicted node is again converted to midi data and all predicted notes form a new music as a result. I trained the RNN model by these midi files and saved them again in google drive to load it when I want to generate a music without needing training since it takes 40 minutes on average. 

The most crucial part is how I link these two tasks to each other. For this purpose, I created four different RNN models which each are trained by specific music sets that reflect the same emotion. Therefore, I have 4 different RNN models which can generate happy, sad, angry and relaxed music. Since I had neutral emotion in emotion recognition task, I took neutral emotion as relaxed emotion in music generation part. Basically, I took the emotion with the highest output probability from CNN in the emotion recognition task. Based on this emotion, the corresponding RNN model which generates the music with the same emotion is used and a new music is generated as output. One last detail which is worth mentioning is that the followed procedures to generate the music sometimes give unrecognizable midi files. Because of this reason, I played with the step and duration values and expanded them by multiplying with constant values. As a result, we get a new generated music which lasts 40 seconds on average. These midi files can reflect the emotions for specific emotions. When happy music is generated, it contains more lively notes. If it is sad music, then it reflects that emotion with the current notes in music. The CNN model does not give the correct classification all the time since it is stuck on one emotion sometimes, probably because of overfitting or poor data quality.
